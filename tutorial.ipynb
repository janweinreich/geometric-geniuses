{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feat2LLM\n",
    "\n",
    "\n",
    "In this tutorial, you will learn how to generate a string-based representation for any numerical feature vector.\n",
    "\n",
    "<img src=\"scheme_new.png\" width=\"90%\" height=\"40%\" />\n",
    "\n",
    "Steps 1 to 3 in the diagram are all executed in the next cell, where ethanol is the target molecule.\n",
    "But let us break down what happens in each step.\n",
    "\n",
    "1) First we download the MD trajectory data and store it\n",
    "\n",
    "2) Based on the molecular geometries we generate representation vectors, here the MBDF representation [1]. To further compress the representation, we perform a dimensionality reduction, here down to 10 components\n",
    "\n",
    "3) Finally the compressed (numerical!) representation vectors are saved to disk\n",
    "\n",
    "\n",
    "[1] Danish Khan, Stefan Heinen, O. Anatole von Lilienfeld; Kernel-based quantum machine learning at record rate: Many-body distribution functionals as compact representations. J. Chem. Phys. 21 July 2023; 159 (3): 034106. https://doi.org/10.1063/5.0152215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from Feat2LLM.load_data import SmallMolTraj\n",
    "\n",
    "mol = \"ethanol\"\n",
    "smallMol = SmallMolTraj(mol)\n",
    "smallMol.get_data()\n",
    "smallMol.gen_representation(n_components=10)\n",
    "smallMol.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the liberty and inspect some of the attributes closer, such as the molecular geometries\n",
    "`R` and total energies `E`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallMol.R, smallMol.R.shape, smallMol.E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for the representation vector `cMBDF` as well as the version in fewer dimensions `cMBDF_trans`, are saved in the `results` attribute. Note that `y` is the same as `E`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallMol.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we just visualize the first two dimensions of the representation vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = smallMol.results[\"cMBDF_trans\"], smallMol.results[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first two columns of X\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X = X[:, :2]\n",
    "\n",
    "# Plotting the density plot using seaborn\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=\"viridis\", s=50, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"$X_{1}$\")\n",
    "plt.ylabel(\"$X_{2}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we perform the last step before fitting the model, we convert the numerical vectors to string representations. We shift the energy scale by the energy of the minimal conformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Feat2LLM.vec2str import ZipFeaturizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X = smallMol.results[\"cMBDF_trans\"]\n",
    "y = smallMol.results[\"y\"]\n",
    "\n",
    "y_min = np.min(y)\n",
    "y+=-y_min \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "converter = ZipFeaturizer(n_bins=300)\n",
    "\n",
    "X_train = converter.bin_vectors(X_train)\n",
    "X_test = converter.bin_vectors(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Feat2LLM.roberta_finetuning import write_data_to_json, load_JSON_data, MoleculeDataset\n",
    "\n",
    "# change the filename depending on the dataset\n",
    "write_data_to_json(X_train, y_train, 'train.json')\n",
    "write_data_to_json(X_test, y_test, 'test.json')\n",
    "\n",
    "data = load_JSON_data(\"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW \n",
    "\n",
    "# Split the data into training and test sets (modify as needed if already split)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "tokenizer       = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset   = MoleculeDataset(train_data, tokenizer)\n",
    "test_dataset    = MoleculeDataset(test_data, tokenizer)\n",
    "\n",
    "# Define the custom model with a regression head\n",
    "class RobertaForRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.regression_head = nn.Linear(self.roberta.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.regression_head(sequence_output)\n",
    "        return logits\n",
    "\n",
    "# Set device: Apple/NVIDIA/CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = RobertaForRegression().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "# DataLoader setup\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(2):  # Number of epochs\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(inputs, mask).squeeze(-1)\n",
    "        loss = nn.MSELoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(inputs, mask).squeeze(-1)\n",
    "        loss = nn.MSELoss()(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}\")\n",
    "\n",
    "# Save model and optimizer state\n",
    "def save_model(model, optimizer, epoch, loss, filepath):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, filepath)\n",
    "\n",
    "# Assuming you want to save the model after training\n",
    "model.eval()\n",
    "\n",
    "if not os.path.exists('save_models'):\n",
    "    os.makedirs('save_models')\n",
    "\n",
    "save_model(model, optimizer, epoch, loss.item(), \"regression.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
