{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feat2LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import Feat2LLM\n",
    "from Feat2LLM.load_data import SmallMolTraj\n",
    "\n",
    "mol = \"ethanol\"\n",
    "smallMol = SmallMolTraj(mol)\n",
    "smallMol.get_data()\n",
    "smallMol.gen_representation(n_components=10)\n",
    "smallMol.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallMol.R, smallMol.R.shape, smallMol.E\n",
    "smallMol.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Feat2LLM.vec2str import ZipFeaturizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = smallMol.results[\"cMBDF_trans\"]\n",
    "y = smallMol.results[\"y\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "converter = ZipFeaturizer(n_bins=300) #<--- you can change if does not work\n",
    "\n",
    "X_train = converter.bin_vectors(X_train)\n",
    "X_test = converter.bin_vectors(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Feat2LLM.roberta_finetuning import write_data_to_json, load_JSON_data, MoleculeDataset\n",
    "\n",
    "# change the filename depending on the dataset\n",
    "write_data_to_json(X_train, y_train, 'train.json')\n",
    "write_data_to_json(X_test, y_test, 'test.json')\n",
    "\n",
    "data = load_JSON_data(\"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW \n",
    "\n",
    "# Split the data into training and test sets (modify as needed if already split)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "tokenizer       = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset   = MoleculeDataset(train_data, tokenizer)\n",
    "test_dataset    = MoleculeDataset(test_data, tokenizer)\n",
    "\n",
    "# Define the custom model with a regression head\n",
    "class RobertaForRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.regression_head = nn.Linear(self.roberta.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.regression_head(sequence_output)\n",
    "        return logits\n",
    "\n",
    "# Set device: Apple/NVIDIA/CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model = RobertaForRegression().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "# DataLoader setup\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(2):  # Number of epochs\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(inputs, mask).squeeze(-1)\n",
    "        loss = nn.MSELoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(inputs, mask).squeeze(-1)\n",
    "        loss = nn.MSELoss()(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}\")\n",
    "\n",
    "# Save model and optimizer state\n",
    "def save_model(model, optimizer, epoch, loss, filepath):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, filepath)\n",
    "\n",
    "# Assuming you want to save the model after training\n",
    "model.eval()\n",
    "\n",
    "if not os.path.exists('save_models'):\n",
    "    os.makedirs('save_models')\n",
    "\n",
    "save_model(model, optimizer, epoch, loss.item(), \"regression.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
